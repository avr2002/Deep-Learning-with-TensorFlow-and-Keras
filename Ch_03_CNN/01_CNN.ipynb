{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abf9dfc-4e4c-4b8b-8dc6-e53c391eb2e7",
   "metadata": {
    "id": "2abf9dfc-4e4c-4b8b-8dc6-e53c391eb2e7"
   },
   "source": [
    "***Reference:***\n",
    "\n",
    "**Kapoor, Amita; Gulli, Antonio; Pal, Sujit. Deep Learning with TensorFlow and Keras: Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition . Packt Publishing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f55bf-8027-4a4b-9deb-524e330d7ba9",
   "metadata": {
    "id": "f58f55bf-8027-4a4b-9deb-524e330d7ba9"
   },
   "source": [
    "# Chapter 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a554c-8787-44ed-a772-6264816ecb96",
   "metadata": {
    "id": "266a554c-8787-44ed-a772-6264816ecb96",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Spatial Structure in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58ba81-9e62-4116-aff1-03b10b0ca044",
   "metadata": {
    "id": "db58ba81-9e62-4116-aff1-03b10b0ca044"
   },
   "source": [
    "While classifying the MNIST handwritten characters using Deep Neural Network, each pixel in the input image has been assigned to a neuron for a total of 784 (28 x 28 pixels) input neurons.\n",
    "\n",
    "**However, this strategy does not leverage the spatial structure and relationships between each image.**\n",
    "\n",
    "In particular, this piece of code is a dense network that transforms the [bitmap](https://www.google.com/search?q=bitmap&oq=bitmap&aqs=chrome..69i57j0i433i512j0i512l8.1399j0j7&sourceid=chrome&ie=UTF-8) representing each written digit into a flat vector where the local spatial structure is removed. **Removing the spatial structure is a problem because important information is lost:**\n",
    "\n",
    "```\n",
    "# X_train in 60000 rows of 28x28 vlaues --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(60000, 784)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d82352-b728-428b-8068-25900eee7ed3",
   "metadata": {
    "id": "c8d82352-b728-428b-8068-25900eee7ed3",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.Deep Convolutional Neural Network(DCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d75e9-0738-4503-976e-e39f3177cbc6",
   "metadata": {
    "id": "ae1d75e9-0738-4503-976e-e39f3177cbc6"
   },
   "source": [
    "A Deep Convolutional Neural Network (DCNN) consists of many neural network layers. Two different types of layers,\n",
    ">**convolutional and pooling (i.e., subsampling), are typically alternated**\n",
    "\n",
    "The depth of each filter increases from left to right in the network. The last stage is typically made of one or more fully connected layers.\n",
    "<div align='center'>\n",
    "    <img src='images/dcnn.png'/>\n",
    "</div>\n",
    "\n",
    "***There are three key underlying concepts for ConvNets:***\n",
    "- **local receptive fields**,\n",
    "- **shared weights**, and\n",
    "- **pooling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c6b04-9d2f-4436-8710-896ef60df27a",
   "metadata": {
    "id": "550c6b04-9d2f-4436-8710-896ef60df27a",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1 **Local Receptive Fields**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf1ac2-6d6f-497a-b530-510223837977",
   "metadata": {
    "id": "52cf1ac2-6d6f-497a-b530-510223837977"
   },
   "source": [
    "If we want to preserve the spatial information of an image or other form of data, then it is convenient to represent each image with a matrix of pixels.\n",
    "\n",
    "Given this, a simple way to encode the local structure is to connect a submatrix of adjacent input neurons into one single hidden neuron belonging to the next layer. That single hidden neuron represents one local receptive field.\n",
    "\n",
    "Note that this operation is named convolution, and this is where the name for this type of network is derived.\n",
    "\n",
    ">**You can think about convolution as the treatment of a matrix by another matrix, referred to as a kernel.**\n",
    "\n",
    "Of course, we can encode more information by having overlapping submatrices. For instance, let's suppose that the size of every single submatrix is 5 x 5 and that those submatrices are used with MNIST images of 28 x 28 pixels. Then we will be able to generate 24 x 24 local receptive field neurons in the hidden layer. In fact, it is possible to slide the submatrices by only 23 positions before touching the borders of the images.\n",
    "\n",
    "\n",
    "In TensorFlow,\n",
    "- the number of pixels along one edge of the kernel, or submatrix, is the **kernel size**, and\n",
    "- the **stride length** is the number of pixels by which the kernel is moved at each step in the convolution.\n",
    "\n",
    "\n",
    "Let's define the feature map from one layer to another. Of course, we can have multiple feature maps that learn independently from each hidden layer. For example, we can start with 28 x 28 input neurons for processing MNIST images, and then define k feature maps of size 24 x 24 neurons each (again with shape of 5 x 5) in the next hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09206174-d14c-403f-9fb3-660e5e496860",
   "metadata": {
    "id": "09206174-d14c-403f-9fb3-660e5e496860",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2 **Shared Weights and Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c11a01-eb3a-4200-8b73-b58998ea7c61",
   "metadata": {
    "id": "77c11a01-eb3a-4200-8b73-b58998ea7c61"
   },
   "source": [
    "Let's suppose that we want to move away from the pixel representation in a raw image, by gaining the ability to detect the same feature independently from the location where it is placed in the input image.\n",
    "\n",
    "A simple approach is to use the same set of weights and biases for all the neurons in the hidden layers.\n",
    "\n",
    "In this way, each layer will learn a set of position-independent latent features derived from the image, bearing in mind that a layer consists of a set of kernels in parallel, and each kernel only learns one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db697d-152b-4c83-a560-d346af07f7d4",
   "metadata": {
    "id": "d6db697d-152b-4c83-a560-d346af07f7d4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **1.3 Example** : `Convolutional`, `Padding` & `Stride`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59bf30f-7865-402e-a288-f54a13e4f82a",
   "metadata": {
    "id": "a59bf30f-7865-402e-a288-f54a13e4f82a"
   },
   "source": [
    "> **One simple way to understand convolution is to think about a sliding window function applied to a matrix.**\n",
    "\n",
    "In the following example, given the input matrix **J** and the kernel **K**, we get the convolved output.\n",
    "\n",
    "The $3 \\times 3$ **kernel K**(sometimes called the **filter or feature detector**) is multiplied elementwise with the input matrix to get one cell in the output matrix. All the other cells are obtained by sliding the window over I:\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/conv2d_ex.png' title='Conv2D_Example'/>\n",
    "    <img src='images/conv2d.gif' title='Conv2D_GIF'/>\n",
    "</div>\n",
    "\n",
    "In this example, we decided to stop the sliding window as soon as we touch the borders of **J**(so the output is $3 \\times 3$).\n",
    "\n",
    "* **\n",
    "\n",
    "**Padding:**\n",
    ">Alternatively, we could have chosen to pad the input with zeros (so that the output would have been $5 \\times 5$). This decision relates to the **padding** choice adopted. Note that kernel depth is equal to input depth (channel).\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/conv2d_padding_no_stride.gif' title='Conv2D_With-Padding_No-Stride'/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "**Stride:**\n",
    ">Another choice is about how far along we slide our sliding windows with each step. This is called the **stride** and it can be one or more.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/con2d_padding_and_strides.gif' title='Conv2D_With-Padding_and_Stride'>\n",
    "</div>                                                         \n",
    "\n",
    "\n",
    "A larger stride generates fewer applications of the kernel and a smaller output size, while a smaller stride generates more output and retains more information.\n",
    "\n",
    "* **\n",
    "\n",
    "The size of the filter, the stride, and the type of padding are\n",
    "hyperparameters that can be fine-tuned during the training of\n",
    "the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9befd87-6dcd-4102-a5b6-6ffff736fd46",
   "metadata": {
    "id": "c9befd87-6dcd-4102-a5b6-6ffff736fd46",
    "tags": []
   },
   "source": [
    "### 1.4 ConvNets in Tensorflow\n",
    "\n",
    "```\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "```\n",
    "\n",
    "**This means that we are applying a 3x3 convolution on 28x28 images with 1 input channel (or input filters) resulting in 32 output channels (or output filters).**\n",
    "\n",
    "\n",
    "<!-- Random Example:\n",
    "<div align='center'>\n",
    "    <img src='images/conv2d_pad.gif'>\n",
    "</div>  -->\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd2562-a798-4a7b-8ed1-ec24490cd05e",
   "metadata": {
    "id": "facd2562-a798-4a7b-8ed1-ec24490cd05e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.5 **Pooling Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1a97c-c089-4998-af94-f43aa39aaa60",
   "metadata": {
    "id": "75d1a97c-c089-4998-af94-f43aa39aaa60"
   },
   "source": [
    ">What is a **feature map?**\n",
    ">>**In CNNs, a feature map is the output of a convolutional layer representing specific features in the input image or feature map.**\n",
    "<img src='images/feature_map.png'/>\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/\n",
    "\n",
    "- https://www.geeksforgeeks.org/introduction-convolution-neural-network/?ref=lbp\n",
    "\n",
    "* **\n",
    "\n",
    "- **Pooling Layers are used to summarize the output of a feature map.**\n",
    "\n",
    "**Max Pooling:**\n",
    "- **Max Pooling simply outputs the maximum activation as observed in the region.** In Keras, if we want to define a max pooling layer of size 2 x 2, we write: `model.add(layers.MaxPooling2D((2,2)))`\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/max_pooling.png'/>\n",
    "</div>\n",
    "\n",
    "**Average Pooling:**\n",
    "- **Average Pooling, which simply aggregates a region into the average values of the activations observed in that region.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc6231-9fc6-4779-bbf4-a3104046490e",
   "metadata": {
    "id": "afcc6231-9fc6-4779-bbf4-a3104046490e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ConvNets Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebe914-07e8-4b6c-b237-be4de39d89a4",
   "metadata": {
    "id": "02ebe914-07e8-4b6c-b237-be4de39d89a4"
   },
   "source": [
    "**CNNs apply convolution and pooling operations in one dimension for audio and text data along the time dimension, in two dimensions for images along the (height x width) dimensions, and in three dimensions for videos along the\n",
    "(height x width x time) dimensions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb63a40-035d-4e66-a010-2822bba19271",
   "metadata": {
    "id": "5eb63a40-035d-4e66-a010-2822bba19271"
   },
   "source": [
    "- **Convolution operation**\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/conv.png\" title=\"Convolution operation\"/>\n",
    "</div>  \n",
    "\n",
    "* **\n",
    "\n",
    "- **Max pool operation**\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/max_pool.png\" title=\"Max pool operation\"/>\n",
    "</div>  \n",
    "\n",
    "* **\n",
    "- [Kernel_(image_processing) - Wikipedia](https://en.wikipedia.org/wiki/Kernel_(image_processing))\n",
    "\n",
    "- [tensorflow doc - conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "    - Example is good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cf485-d435-47ad-bc7b-baad8aade70e",
   "metadata": {
    "id": "eb5cf485-d435-47ad-bc7b-baad8aade70e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. DCNN: LeNet in Tensorflow using MNIST Dataset\n",
    "\n",
    "- **The core idea of LeNet is to have lower layers alternating convolutional operations with max-pooling operations.**\n",
    "\n",
    "- The convolution operations are based on carefully chosen local receptive fields with shared weights for multiple feature maps.\n",
    "\n",
    "- Then, higher levels are fully connected based on a traditional MLP with hidden layers and softmax as the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac3d32-7802-451e-98b8-71f933322d87",
   "metadata": {
    "id": "d0ac3d32-7802-451e-98b8-71f933322d87"
   },
   "source": [
    "* **\n",
    "1. To define a LeNet in Code, we use `Convolutional2D` Module\n",
    "```\n",
    "tf.keras.layers.Conv2D(filters,\n",
    "                       kernel_size,\n",
    "                       strides=(1, 1),\n",
    "                       padding='valid',...)\n",
    "```\n",
    "\n",
    "- The first parameter is the number of output `filters` in the convolution and the next tuple(`kernal_size`) is the extension of each filter.\n",
    "\n",
    "- Another parameter `padding`:\n",
    "    - `padding = 'valid'` means that the convolution is only computed where the input and the filter fully overlap and therefore the output is smaller than the input,\n",
    "    \n",
    "    - while `padding = 'same'` means that we have an output that is the same size as the input, for which the area around the input is padded with zeros.\n",
    "    \n",
    "* **\n",
    "\n",
    "2. In addition we use `MaxPooling2D` module:\n",
    "```\n",
    "tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "                             strides=(2,2), # default=None\n",
    "                             padding='valid',)\n",
    "```\n",
    "where `pool_size=(2, 2)` is a tuple of 2 integers representing the factors by which the image is vertically and horizontally downscaled. So `(2, 2)` will halve the image in each dimension, and `strides=(2, 2)` is the stride used for\n",
    "processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9896c231-a1aa-421a-820e-800fb593d905",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9896c231-a1aa-421a-820e-800fb593d905",
    "outputId": "03324460-b00b-4cba-de8f-7c40d29780e4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a3d55f-a4e2-4830-9113-b180344ee2ad",
   "metadata": {
    "id": "40a3d55f-a4e2-4830-9113-b180344ee2ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "VALIDATION_SPLIT = 0.20\n",
    "IMG_ROWS, IMG_COLS = 28, 28 # i/p image dimensions\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1) # 1 -> Only one color channel\n",
    "NB_CLASSES = 10 # Output Classes = 10 digits\n",
    "\n",
    "# Define the LeNet Network:\n",
    "class LeNet:\n",
    "    # define the convnet\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        # CONV => RELU => POOL : Stage 1\n",
    "        model.add(keras.layers.Conv2D(filters=20, kernel_size=(5,5),\n",
    "                                      activation='relu'))\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "        # CONV => RELU => POOL : Stage 2\n",
    "        model.add(keras.layers.Conv2D(50, (5,5), activation='relu'))\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "        # Flatten => RELU Layers : Stage 3\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(500, activation='relu'))\n",
    "\n",
    "        # a SOFTMAX classifier\n",
    "        model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07455009-0de4-436a-aae1-5399f96829d1",
   "metadata": {
    "id": "07455009-0de4-436a-aae1-5399f96829d1"
   },
   "source": [
    "**Visualizing above defined LeNet Architechture:**\n",
    "<div align='center'>\n",
    "    <img src='images/lenet.png' title='Visualization of LeNet'/>\n",
    "</div>\n",
    "\n",
    "- **Stage 1:**\n",
    "    - **We have a first convolutional stage with ReLU activations followed by max pooling.** Our network will learn 20 convolutional filters, each one of which has a size of 5x5. The output dimension is the same as the input shape, so it will be 28 x 28. Note that since `Convolutiona12D` is the first stage of our pipeline, we are also required to define its `input_shape`.\n",
    "    \n",
    "```\n",
    "# CONV => RELU => POOL : Stage 1\n",
    "model.add(keras.layers.Conv2D(filters=20, kernel_size=(5,5),\n",
    "                              activation='relu',\n",
    "                              input_shape=input_shape))\n",
    "                              \n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2),\n",
    "                                    strides=(2,2)))\n",
    "        \n",
    "```\n",
    "\n",
    "* **\n",
    "\n",
    "- **Stage 2:**\n",
    "    - **Then there is a 2nd convolutional stage with ReLU activations, followed again by a max pooling layer.** In this case, we increase the number of convolutional filters learned to 50 from the previous 20. **Increasing the number of filters in deeper layers is a common technique in deep learning.**\n",
    "    \n",
    "```\n",
    "# CONV => RELU => POOL : Stage 2\n",
    "model.add(keras.layers.Convolution2D(50, (5,5), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "```\n",
    "\n",
    "* **\n",
    "\n",
    "- **Stage 3:**\n",
    "    - Then we have a pretty standard flattening and a dense network of 500 neurons, followed by a softmax classifier with 10 classes:\n",
    "    \n",
    "```\n",
    "# Flatten => RELU Layers : Stage 3\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(500, activation='relu'))\n",
    "        \n",
    "# a SOFTMAX classifier\n",
    "model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2cdfa3-849b-45bf-8e34-db9a4333e73a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb2cdfa3-849b-45bf-8e34-db9a4333e73a",
    "outputId": "74074760-c293-4b88-c893-99618af4327a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# cast\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f63f64-80f4-4d54-8a04-1c82e185d80d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1f63f64-80f4-4d54-8a04-1c82e185d80d",
    "outputId": "c27d197a-625f-4386-a120-399d7674441c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 20)        520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 50)          25050     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               400500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "\n",
    "# Comile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8532b4ef-cc3b-451c-892b-d31ee9daeacf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8532b4ef-cc3b-451c-892b-d31ee9daeacf",
    "outputId": "b343390f-4497-49f3-e29a-d2a52e66beb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 5s 4ms/step - loss: 0.2056 - accuracy: 0.9395 - val_loss: 0.0714 - val_accuracy: 0.9788\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.0533 - val_accuracy: 0.9841\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.0397 - val_accuracy: 0.9883\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0259 - accuracy: 0.9916 - val_loss: 0.0415 - val_accuracy: 0.9877\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.0350 - val_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# use TensorBoard, princess Aurora!\n",
    "callbacks = [\n",
    "    # Write TensorBoard logs to `./logs` directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a92a47-e4b2-42a6-a867-142f14ae1f67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94a92a47-e4b2-42a6-a867-142f14ae1f67",
    "outputId": "d46f122c-8a7a-425f-aba7-a1680f14fc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0167 - accuracy: 0.9951\n",
      "\n",
      "Train score: 0.016713107004761696\n",
      "Train accuracy: 0.9951333403587341\n",
      "\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0275 - accuracy: 0.9912\n",
      "\n",
      "Test score: 0.02745833992958069\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(X_train, y_train, verbose=VERBOSE)\n",
    "print(f\"\\nTrain score: {train_score[0]}\")\n",
    "print(f'Train accuracy: {train_score[1]}\\n')\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897d857-06ce-4138-80b2-1d5ab2f1adeb",
   "metadata": {},
   "source": [
    " * **"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
